1. Does your network reach 0 training error? 
Ans: Yes

2. Can you make your program into stochastic gradient descent (SGD)?
Ans: Yes

3. Does SGD give lower test error than full gradient descent?
Ans: Yes

4. What happens if change the activation to sign? Will the same algorithm
work? If not what will you change to make the algorithm converge to a local
minimum?
Ans: No. Derivative of sign is zero, yes we have to change the algorithms.